# -*- coding: utf-8 -*-
"""ml_recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ssXkTl22Y_4VithXj9we6Z0ny8M-ImcX
"""

#  !pip install streamlit
#  !pip install plotly
# pip install -r requirements.txt
# pip install --upgrade pip

import streamlit as st
from PIL import Image
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# –ò–∫–æ–Ω–∫–∞ –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫
st.set_page_config(
    page_title="ML Algorithm Recommender",
    page_icon="üß†",
    layout="wide"
)

# –õ–æ–≥–æ—Ç–∏–ø –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
# col1, col2 = st.columns([1, 4])
# with col1:
#     st.image("https://miro.medium.com/v2/resize:fit:1200/1*Y57k4A2rC1JkUX3x-Cg_TQ.png", width=100)
# with col2:
#     st.title("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º ML")
#     st.caption("–í—ã–±–µ—Ä–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏, –∏ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")

algorithms_db = {
    # –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏
    "Linear Regression": {
        "task_type": ["regression"],
        "data_size": "any",
        "feature_types": ["numeric"],
        "interpretability": "high",
        "training_speed": "fast",
        "robust_to_noise": "low",
        "handles_missing_data": "no",
        "gpu_support": "no",
        "notes": "–ü—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π. –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º.",
        "hyperparameters": {
            "default": {"fit_intercept": True},
            "tips": "–î–ª—è –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ –¥–æ–±–∞–≤—å—Ç–µ ridge/lasso —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é"
        },
        "documentation": {
            "scikit-learn": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html",
            "concept": "https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares"
        }
    },
    "Logistic Regression": {
        "task_type": ["classification"],
        "data_size": "any",
        "feature_types": ["numeric", "categorical (encoded)"],
        "interpretability": "high",
        "training_speed": "fast",
        "robust_to_noise": "medium",
        "handles_missing_data": "no",
        "gpu_support": "no",
        "notes": "–•–æ—Ä–æ—à –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –¢—Ä–µ–±—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.",
        "hyperparameters": {
            "default": {"C": 1.0, "penalty": "l2", "max_iter": 100},
            "tips": "–£–º–µ–Ω—å—à–∏—Ç–µ C –¥–ª—è —Å–∏–ª—å–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏. –î–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ class_weight='balanced'"
        },
        "documentation": {
            "scikit-learn": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html",
            "guide": "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
        }

    },

    # –î–µ—Ä–µ–≤—å—è –∏ –∞–Ω—Å–∞–º–±–ª–∏
    "Decision Tree": {
        "task_type": ["classification", "regression"],
        "data_size": "small-medium",
        "feature_types": ["numeric", "categorical"],
        "interpretability": "high",
        "training_speed": "fast",
        "robust_to_noise": "low",
        "handles_missing_data": "yes",
        "gpu_support": "no",
        "notes": "–°–∫–ª–æ–Ω–µ–Ω –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é. –•–æ—Ä–æ—à –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö —Ä–µ—à–µ–Ω–∏–π.",
        "hyperparameters": {
            "default": {
                "criterion": "gini",  # –∏–ª–∏ "entropy" –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                "max_depth": None,
                "min_samples_split": 2
            },
            "important": {
                "max_depth": "–ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (–Ω–∞—á–Ω–∏—Ç–µ —Å 3-5 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)",
                "min_samples_leaf": "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –ª–∏—Å—Ç–µ (—É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –º–æ–¥–µ–ª–∏)",
                "max_features": "–ß–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è ('sqrt' –¥–ª—è —Ä–∞–Ω–¥–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–µ—Ä–µ–≤—å–µ–≤)"
            },
            "tips": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ plot_tree() –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ—à–µ–Ω–∏–π"
        },
        "documentation": {
            "scikit-learn": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html",
            "visualization": "https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use"
        }
    },
    "Random Forest": {
        "task_type": ["classification", "regression"],
        "data_size": "medium-large",
        "feature_types": ["numeric", "categorical"],
        "interpretability": "medium",
        "training_speed": "medium",
        "robust_to_noise": "high",
        "handles_missing_data": "yes",
        "gpu_support": "no",
        "notes": "–£—Å—Ç–æ–π—á–∏–≤ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é. –ú–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏.",
         "hyperparameters": {
            "default": {"n_estimators": 100, "max_depth": None},
            "important": {
                "max_depth": "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (–Ω–∞—á–Ω–∏—Ç–µ —Å 5-10)",
                "min_samples_leaf": "–£–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ –¥–ª—è –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º"
            }
        },
        "documentation": {
            "scikit-learn": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html",
            "user guide": "https://scikit-learn.org/stable/modules/ensemble.html#forest"
        }
    },
    "Gradient Boosting (XGBoost/LightGBM/CatBoost)": {
        "task_type": ["classification", "regression"],
        "data_size": "any",
        "feature_types": ["numeric", "categorical"],
        "interpretability": "low",
        "training_speed": "medium (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)",
        "robust_to_noise": "high",
        "handles_missing_data": "yes (–æ—Å–æ–±–µ–Ω–Ω–æ CatBoost)",
        "gpu_support": "yes",
        "notes": "–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–∏ –∞–Ω—Å–∞–º–±–ª–µ–π. –¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.",
        "hyperparameters": {
            "default": {"n_estimators": 100, "learning_rate": 0.1},
            "tuning_strategy": "–°–Ω–∞—á–∞–ª–∞ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ n_estimators –∏ learning_rate, –∑–∞—Ç–µ–º max_depth/min_child_weight"
        },
        "documentation": {
            "XGBoost": "https://xgboost.readthedocs.io/",
            "LightGBM": "https://lightgbm.readthedocs.io/",
            "CatBoost": "https://catboost.ai/en/docs/"
        }
    },

    # SVM –∏ —è–¥–µ—Ä–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    "SVM": {
        "task_type": ["classification", "regression"],
        "data_size": "small-medium",
        "feature_types": ["numeric"],
        "interpretability": "low",
        "training_speed": "slow",
        "robust_to_noise": "medium",
        "handles_missing_data": "no",
        "gpu_support": "partial (cuML)",
        "notes": "–•–æ—Ä–æ—à –¥–ª—è –º–∞–ª—ã—Ö datasets —Å —á—ë—Ç–∫–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏. –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –º–∞—Å—à—Ç–∞–±—É –¥–∞–Ω–Ω—ã—Ö.",
        "hyperparameters": {
            "default": {
                "C": 1.0,
                "kernel": "rbf",
                "gamma": "scale"
            },
            "important": {
                "C": "–ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (–º–µ–Ω—å—à–µ = —Å–∏–ª—å–Ω–µ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)",
                "kernel": "–í—ã–±–æ—Ä —è–¥—Ä–∞: 'linear', 'poly', 'rbf', 'sigmoid'",
                "gamma": "–í–ª–∏—è–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ ('scale' –∏–ª–∏ 'auto')"
            },
            "tips": "–î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ LinearSVM"
        },
        "documentation": {
            "scikit-learn": "https://scikit-learn.org/stable/modules/svm.html"
        }
    },

    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    "K-Means": {
        "task_type": ["clustering"],
        "data_size": "any",
        "feature_types": ["numeric"],
        "interpretability": "medium",
        "training_speed": "fast",
        "robust_to_noise": "low",
        "handles_missing_data": "no",
        "gpu_support": "yes (cuML)",
        "notes": "–¢—Ä–µ–±—É–µ—Ç –∑–∞–¥–∞–Ω–∏—è —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (k). –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º.",
        "hyperparameters": {
            "default": {
                "n_clusters": 8,
                "init": "k-means++",
                "n_init": 10
            },
            "important": {
                "n_clusters": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ elbow method –¥–ª—è –ø–æ–¥–±–æ—Ä–∞)",
                "init": "–ú–µ—Ç–æ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤ ('random' –∏–ª–∏ 'k-means++')"
            },
            "tips": "–í—Å–µ–≥–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π"
        },
        "documentation": {
            "scikit-learn": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
        }
    },
    "DBSCAN": {
        "task_type": ["clustering"],
        "data_size": "small-medium",
        "feature_types": ["numeric"],
        "interpretability": "low",
        "training_speed": "medium",
        "robust_to_noise": "high",
        "handles_missing_data": "no",
        "gpu_support": "partial",
        "notes": "–ù–µ —Ç—Ä–µ–±—É–µ—Ç –∑–∞–¥–∞–Ω–∏—è —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –•–æ—Ä–æ—à –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å —à—É–º–æ–º.",
        "hyperparameters": {
            "default": {
                "eps": 0.5,
                "min_samples": 5
            },
            "important": {
                "eps": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Å–æ—Å–µ–¥—è–º–∏",
                "min_samples": "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ"
            },
            "tips": "–•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å —à—É–º–æ–º –∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ —Ä–∞–∑–Ω–æ–π —Ñ–æ—Ä–º—ã"
        },
        "documentation": {
            "scikit-learn": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
        }
    },

    # –ù–µ–π—Ä–æ—Å–µ—Ç–∏
    "MLP (Neural Network)": {
        "task_type": ["classification", "regression"],
        "data_size": "large",
        "feature_types": ["numeric", "categorical (encoded)"],
        "interpretability": "very low",
        "training_speed": "slow (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)",
        "robust_to_noise": "medium",
        "handles_missing_data": "no",
        "gpu_support": "yes",
        "notes": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.",
        "hyperparameters": {
            "default": {"hidden_layer_sizes": (100,), "activation": "relu"},
            "architecture_tips": "–ù–∞—á–∏–Ω–∞–π—Ç–µ —Å 1-2 —Å–ª–æ—ë–≤. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ BatchNorm –∏ Dropout"
        },
        "documentation": {
            "scikit-learn": "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html",
            "keras": "https://keras.io/api/layers/core_layers/dense/"
        }
    },
    "CNN (–¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)": {
        "task_type": ["classification", "regression"],
        "data_size": "very large",
        "feature_types": ["images"],
        "interpretability": "very low",
        "training_speed": "very slow (–±–µ–∑ GPU –ø–æ—á—Ç–∏ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω)",
        "robust_to_noise": "high",
        "handles_missing_data": "no",
        "gpu_support": "required",
        "notes": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –¢—Ä–µ–±—É–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
        "hyperparameters": {
            "default_architecture": "Conv2D(32) -> MaxPooling -> Conv2D(64) -> Flatten -> Dense(128)",
            "transfer_learning": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (ResNet, EfficientNet)"
        },
        "documentation": {
            "keras": "https://keras.io/api/layers/convolution_layers/convolution2d/",
            "pytorch": "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"
        }
    },
    "RNN/LSTM (–¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤)": {
        "task_type": ["classification", "regression"],
        "data_size": "large",
        "feature_types": ["time_series", "text"],
        "interpretability": "very low",
        "training_speed": "very slow",
        "robust_to_noise": "medium",
        "handles_missing_data": "no",
        "gpu_support": "required",
        "notes": "–î–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (—Ç–µ–∫—Å—Ç, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã). –°–ª–æ–∂–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ.",
        "hyperparameters": {
            "default_architecture": {
                "units": 64,
                "dropout": 0.2,
                "recurrent_dropout": 0.2,
                "batch_size": 32
            },
            "important": {
                "units": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ LSTM-—Å–ª–æ–µ (–Ω–∞—á–Ω–∏—Ç–µ —Å 32-128)",
                "return_sequences": "True –¥–ª—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö RNN",
                "optimizer": "Adam —Å learning_rate ~0.001"
            },
            "tips": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Bidirectional LSTM –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á"
        },
        "documentation": {
            "keras": "https://keras.io/api/layers/recurrent_layers/lstm/",
            "pytorch": "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
        }
    },

    # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤
    "Naive Bayes": {
        "task_type": ["classification"],
        "data_size": "any",
        "feature_types": ["text", "categorical"],
        "interpretability": "high",
        "training_speed": "very fast",
        "robust_to_noise": "medium",
        "handles_missing_data": "yes",
        "gpu_support": "no",
        "notes": "–•–æ—Ä–æ—à –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å–ø–∞–º-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏. –°–∏–ª—å–Ω–æ —É–ø—Ä–æ—â–∞–µ—Ç –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è.",
        "hyperparameters": {
            "default": {
                "alpha": 1.0,
                "fit_prior": True
            },
            "important": {
                "alpha": "–ü–∞—Ä–∞–º–µ—Ç—Ä —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (–º–µ–Ω—å—à–µ = —Å–∏–ª—å–Ω–µ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)",
                "var_smoothing": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è GaussianNB)"
            },
            "tips": "MultinomialNB –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
        },
        "documentation": {
            "scikit-learn": "https://scikit-learn.org/stable/modules/naive_bayes.html",
            "comparison": "https://scikit-learn.org/stable/modules/naive_bayes.html#comparison-of-naive-bayes-models"
        }
    },
    "Transformer (BERT/GPT)": {
        "task_type": ["classification", "regression"],
        "data_size": "very large",
        "feature_types": ["text"],
        "interpretability": "very low",
        "training_speed": "extremely slow",
        "robust_to_noise": "high",
        "handles_missing_data": "no",
        "gpu_support": "required",
        "notes": "State-of-the-art –¥–ª—è NLP. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ Hugging Face).",
        "hyperparameters": {
            "default_pretrained": {
                "model_name": "bert-base-uncased",
                "learning_rate": 2e-5,
                "batch_size": 16
            },
            "important": {
                "max_length": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–æ–±—ã—á–Ω–æ 128-512)",
                "num_train_epochs": "3-5 —ç–ø–æ—Ö –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è fine-tuning",
                "warmup_steps": "10% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —à–∞–≥–æ–≤"
            },
            "tips": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Hugging Face Transformers —Å GPU –∏ mixed-precision"
        },
        "documentation": {
            "HuggingFace": "https://huggingface.co/docs/transformers/index",
            "BERT paper": "https://arxiv.org/abs/1810.04805",
            "GPT paper": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
        }
    }
}

# def get_user_input():
#     """–°–æ–±–∏—Ä–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—á–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
#     print("–û–ø–∏—à–∏—Ç–µ –≤–∞—à—É –∑–∞–¥–∞—á—É –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:")
#     task_type = input("–¢–∏–ø –∑–∞–¥–∞—á–∏ (classification/regression/clustering): ").strip().lower()
#     data_size = input("–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (small/medium/large/very large): ").strip().lower()
#     feature_types = input("–¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (numeric/categorical/text/images/time_series): ").strip().lower().split(", ")
#     interpretability = input("–í–∞–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å? (yes/no): ").strip().lower() == "yes"
#     training_speed = input("–í–∞–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è? (yes/no): ").strip().lower() == "yes"
#     gpu_available = input("–ï—Å—Ç—å –ª–∏ GPU? (yes/no): ").strip().lower() == "yes"
#     missing_data = input("–ï—Å—Ç—å –ª–∏ –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö? (yes/no): ").strip().lower() == "yes"

#     return {
#         "task_type": task_type,
#         "data_size": data_size,
#         "feature_types": feature_types,
#         "interpretability": interpretability,
#         "training_speed": training_speed,
#         "gpu_available": gpu_available,
#         "missing_data": missing_data
#     }

def recommend_algorithm(user_input, db):
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã."""
    suitable_algorithms = []

    for algo_name, algo_params in db.items():
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        if user_input["task_type"] not in algo_params["task_type"]:
            continue

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        compatible_features = True
        for ft in user_input["feature_types"]:
            if ft not in " ".join(algo_params["feature_types"]):
                compatible_features = False
                break
        if not compatible_features:
            continue

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—Ä—É–≥–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if user_input["gpu_available"] and algo_params["gpu_support"] == "no":
            continue
        if user_input["missing_data"] and algo_params["handles_missing_data"] == "no":
            continue

        suitable_algorithms.append((algo_name, algo_params))

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
    if user_input["interpretability"]:
        priority_order = ["high", "medium", "low", "very low"]
        suitable_algorithms.sort(key=lambda x: priority_order.index(x[1]["interpretability"]))
    elif user_input["training_speed"]:
        priority_order = ["very fast", "fast", "medium", "slow", "very slow", "extremely slow"]
        suitable_algorithms.sort(key=lambda x: priority_order.index(x[1]["training_speed"]))

    return suitable_algorithms

# def print_recommendations(recommendations):
#     """–í—ã–≤–æ–¥–∏—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö, –≤–∫–ª—é—á–∞—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
#     if not recommendations:
#         print("\n–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤. –£—Ç–æ—á–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—á–∏.")
#         return

#     print("\n=== –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã ===")
#     for i, (algo, params) in enumerate(recommendations, 1):
#         print(f"\n{i}. {algo}")
#         print(f"   - –¢–∏–ø –∑–∞–¥–∞—á–∏: {', '.join(params['task_type'])}")
#         print(f"   - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å: {params['interpretability']}")
#         print(f"   - –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {params['training_speed']}")
#         print(f"   - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ GPU: {params['gpu_support']}")
#         print(f"   - –†–∞–±–æ—Ç–∞ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏: {params['handles_missing_data']}")

#         # –í—ã–≤–æ–¥ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
#         if "hyperparameters" in params:
#             hp = params["hyperparameters"]
#             print(f"\n   –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è {algo}:")

#             if "default" in hp:
#                 print("   - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:", hp["default"])

#             if "important" in hp:
#                 print("   - –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
#                 for param, desc in hp["important"].items():
#                     print(f"     * {param}: {desc}")

#             if "tips" in hp:
#                 print("   - –°–æ–≤–µ—Ç—ã:", hp["tips"])

#         # –í—ã–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
#         if "documentation" in params:
#             print("\n   –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:")
#             for name, url in params["documentation"].items():
#                 print(f"     - {name}: {url}")

#         print(f"   - –ü—Ä–∏–º–µ—á–∞–Ω–∏—è: {params['notes']}")

# if __name__ == "__main__":
#     print("=== –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ ML-–∞–ª–≥–æ—Ä–∏—Ç–º–∞–º ===")
#     user_input = get_user_input()
#     recommendations = recommend_algorithm(user_input, algorithms_db)
#     print_recommendations(recommendations)

def visualize_algorithm_comparison(recommendations):
    """–°–æ–∑–¥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤"""
    if not recommendations:
        return

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    algo_names = [algo[0] for algo in recommendations]
    params_list = [algo[1] for algo in recommendations]

    # 1. –†–∞–¥–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    metrics = ['interpretability', 'training_speed', 'robust_to_noise']
    metric_labels = ['–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å', '–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è', '–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É']

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    metric_mapping = {
        'very high': 5, 'high': 4, 'medium': 3, 'low': 2, 'very low': 1,
        'very fast': 5, 'fast': 4, 'medium': 3, 'slow': 2, 'very slow': 1
    }

    fig1 = go.Figure()

    for algo, params in zip(algo_names, params_list):
        values = []
        for metric in metrics:
            value = params.get(metric, 'medium')
            values.append(metric_mapping.get(value.lower(), 3))

        fig1.add_trace(go.Scatterpolar(
            r=values + values[:1],  # –ó–∞–º—ã–∫–∞–µ–º –∫—Ä—É–≥
            theta=metric_labels + metric_labels[:1],
            fill='toself',
            name=algo
        ))

    fig1.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 5])),
        showlegend=True,
        title='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤',
        height=500
    )

    # 2. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    param_data = []
    for algo, params in zip(algo_names, params_list):
        if 'hyperparameters' in params and 'default' in params['hyperparameters']:
            for param, value in params['hyperparameters']['default'].items():
                param_data.append({
                    'Algorithm': algo,
                    'Parameter': param,
                    'Value': str(value)[:50]  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                })

    if param_data:
        df_params = pd.DataFrame(param_data)
        fig2 = px.bar(df_params,
                     x='Algorithm',
                     y='Value',
                     color='Parameter',
                     barmode='group',
                     title='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é',
                     height=400)
    else:
        fig2 = None

    # 3. –ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    features = set()
    for params in params_list:
        features.update(params.get('feature_types', []))

    feature_matrix = []
    for algo, params in zip(algo_names, params_list):
        row = {'Algorithm': algo}
        for feature in features:
            row[feature] = feature in params.get('feature_types', [])
        feature_matrix.append(row)

    df_features = pd.DataFrame(feature_matrix).set_index('Algorithm')
    fig3 = px.imshow(df_features.T,
                    labels=dict(x="–ê–ª–≥–æ—Ä–∏—Ç–º", y="–¢–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", color="–ü–æ–¥–¥–µ—Ä–∂–∫–∞"),
                    title="–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
                    color_continuous_scale='Blues')

    return fig1, fig2, fig3

def create_parameter_tuner(params):
    """–°–æ–∑–¥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    tuned_params = {}

    if "default" not in params:
        return params

    st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

    for param, default_value in params["default"].items():
        col1, col2 = st.columns([1, 3])

        with col1:
            st.markdown(f"**{param}**")

        with col2:
            param_type = type(default_value)

            if param_type == bool:
                tuned_params[param] = st.checkbox(
                    "", value=default_value, key=param,
                    help=params.get("important", {}).get(param, "")
                )
            elif param_type == int:
                min_val = max(0, default_value - 10) if default_value > 10 else 0
                max_val = default_value + 10
                tuned_params[param] = st.slider(
                    "", min_value=min_val, max_value=max_val,
                    value=default_value, key=param,
                    help=params.get("important", {}).get(param, "")
                )
            elif param_type == float:
                tuned_params[param] = st.number_input(
                    "", value=float(default_value), key=param,
                    help=params.get("important", {}).get(param, ""),
                    step=0.01
                )
            elif param in ["criterion", "penalty", "kernel"]:
                options = {
                    "criterion": ["gini", "entropy"],
                    "penalty": ["l1", "l2", "elasticnet"],
                    "kernel": ["linear", "poly", "rbf", "sigmoid"]
                }
                tuned_params[param] = st.selectbox(
                    "", options.get(param, [default_value]),
                    key=param,
                    help=params.get("important", {}).get(param, "")
                )
            else:
                tuned_params[param] = st.text_input(
                    "", value=str(default_value), key=param,
                    help=params.get("important", {}).get(param, "")
                )

    if "tips" in params:
        st.info(params["tips"])

    return tuned_params

def generate_algorithm_code(algo_name, tuned_params, params):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–æ—Ç–æ–≤—ã–π Python –∫–æ–¥ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Å —É—á–µ—Ç–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    # –ë–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω –∫–æ–¥–∞
    code_template = {
        "imports": "",
        "data_loading": """
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
data = pd.read_csv('your_data.csv')
X = data.drop('target', axis=1)
y = data['target']

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)
""",
        "preprocessing": "",
        "model_init": "",
        "training": "",
        "evaluation": """
# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞
y_pred = model.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
from sklearn.metrics import classification_report, accuracy_score
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\\nClassification Report:\\n", classification_report(y_test, y_pred))
"""
    }

    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã
    code_template["imports"] = "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n"

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
    if "Linear Regression" in algo_name:
        code_template["imports"] += "from sklearn.linear_model import LinearRegression"
        code_template["model_init"] = f"model = LinearRegression({generate_params(tuned_params)})"
        code_template["evaluation"] = """
from sklearn.metrics import mean_squared_error, r2_score
y_pred = model.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))
"""

    elif "Logistic Regression" in algo_name:
        code_template["imports"] += "from sklearn.linear_model import LogisticRegression"
        code_template["model_init"] = f"model = LogisticRegression({generate_params(tuned_params)})"

    elif "Decision Tree" in algo_name:
        code_template["imports"] += "from sklearn.tree import DecisionTreeClassifier"
        code_template["model_init"] = f"model = DecisionTreeClassifier({generate_params(tuned_params)})"
        code_template["imports"] += "\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt"
        code_template["evaluation"] += """
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤–∞
plt.figure(figsize=(12,8))
plot_tree(model, filled=True, feature_names=X.columns, class_names=True)
plt.show()
"""

    elif "Random Forest" in algo_name:
        code_template["imports"] += "from sklearn.ensemble import RandomForestClassifier"
        code_template["model_init"] = f"model = RandomForestClassifier({generate_params(tuned_params)})"
        code_template["imports"] += "\nimport seaborn as sns"
        code_template["evaluation"] += """
# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
feature_imp = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10,6))
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
plt.show()
"""

    elif "XGBoost" in algo_name or "Gradient Boosting" in algo_name:
        code_template["imports"] += "from xgboost import XGBClassifier"
        code_template["model_init"] = f"model = XGBClassifier({generate_params(tuned_params)})"
        code_template["imports"] += "\nfrom xgboost import plot_importance"
        code_template["evaluation"] += """
# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ XGBoost
plt.figure(figsize=(10,6))
plot_importance(model)
plt.show()
"""

    elif "MLP" in algo_name:
        code_template["imports"] += "from sklearn.neural_network import MLPClassifier"
        code_template["model_init"] = f"model = MLPClassifier({generate_params(tuned_params)})"
        code_template["preprocessing"] = """
# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
"""

    elif "CNN" in algo_name:
        code_template["imports"] = """
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
"""
        code_template["model_init"] = """
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
"""
        code_template["preprocessing"] = """
# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
X_train = X_train.reshape(-1, 28, 28, 1) / 255.0
X_test = X_test.reshape(-1, 28, 28, 1) / 255.0
"""
        code_template["training"] = """
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train, y_train,
                    epochs=10,
                    validation_data=(X_test, y_test))
"""
        code_template["evaluation"] = """
# –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
plt.plot(history.history['accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏')
plt.plot(history.history['val_accuracy'], label='–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
plt.xlabel('–≠–ø–æ—Ö–∏')
plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
plt.legend()
plt.show()
"""

    elif "RNN" in algo_name or "LSTM" in algo_name:
        code_template["imports"] = """
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding
"""
        code_template["model_init"] = f"""
model = Sequential([
    Embedding(input_dim=10000, output_dim=64),
    LSTM({tuned_params.get('units', 64)}),
    Dense(1, activation='sigmoid')
])
"""
        code_template["preprocessing"] = """
# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)
X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=100)
X_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=100)
"""
        code_template["training"] = """
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train, y_train,
                    epochs=5,
                    batch_size=32,
                    validation_data=(X_test, y_test))
"""

    elif "Transformer" in algo_name:
        code_template["imports"] = """
from transformers import pipeline, AutoTokenizer
import torch
"""
        code_template["model_init"] = f"""
model = pipeline('text-classification',
                model='{params["hyperparameters"]["default_pretrained"]["model_name"]}',
                device={0 if params["gpu_support"] == "required" else -1})
"""
        code_template["preprocessing"] = """
# –î–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
"""

    # –°–±–æ—Ä–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–¥–∞
    full_code = f"""{code_template['imports']}

{code_template['data_loading']}

{code_template['preprocessing']}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
{code_template['model_init']}

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
{code_template['training'] if code_template['training'] else 'model.fit(X_train, y_train)'}

{code_template['evaluation']}
"""

    return full_code

def generate_params(params_dict):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç—Ä–æ–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏"""
    return ", ".join([f"{k}={repr(v)}" for k, v in params_dict.items()])

# def generate_algorithm_code(algo_name, tuned_params, params):
#     """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–æ—Ç–æ–≤—ã–π –∫–æ–¥ Python –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞"""
#     base_code = {
#         "imports": "",
#         "model_init": "",
#         "training": "",
#         "prediction": ""
#     }

#     # –û–±—â–∏–µ –∏–º–ø–æ—Ä—Ç—ã
#     base_code["imports"] = "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n"

#     # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
#     if "Linear Regression" in algo_name:
#         base_code["imports"] += "from sklearn.linear_model import LinearRegression\n"
#         base_code["model_init"] = f"model = LinearRegression({generate_params(params(tuned_params))})"

#     elif "Logistic Regression" in algo_name:
#         base_code["imports"] += "from sklearn.linear_model import LogisticRegression\n"
#         base_code["model_init"] = f"model = LogisticRegression({generate_params(params(tuned_params))})"

#     elif "Decision Tree" in algo_name:
#         base_code["imports"] += "from sklearn.tree import DecisionTreeClassifier\n"
#         base_code["model_init"] = f"model = DecisionTreeClassifier({generate_params(params(tuned_params))})"

#     elif "Random Forest" in algo_name:
#         base_code["imports"] += "from sklearn.ensemble import RandomForestClassifier\n"
#         base_code["model_init"] = f"model = RandomForestClassifier({generate_params(params(tuned_params))})"

#     elif "XGBoost" in algo_name:
#         base_code["imports"] += "from xgboost import XGBClassifier\n"
#         base_code["model_init"] = f"model = XGBClassifier({generate_params(params(tuned_params))})"

#     elif "MLP" in algo_name:
#         base_code["imports"] += "from sklearn.neural_network import MLPClassifier\n"
#         base_code["model_init"] = f"model = MLPClassifier({generate_params(params(tuned_params))})"

#     elif "Transformer" in algo_name:
#         base_code["imports"] = "from transformers import pipeline\nimport torch\n"
#         base_code["model_init"] = f"model = pipeline('text-classification', model='{params['hyperparameters']['default_pretrained']['model_name']}')"

#     # –û–±—â–∞—è —á–∞—Å—Ç—å –∫–æ–¥–∞
#     base_code["training"] = """
#     # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
#     data = pd.read_csv('your_data.csv')
#     X = data.drop('target', axis=1)
#     y = data['target']

#     # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#     # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
#     model.fit(X_train, y_train)
#     """
#     base_code["prediction"] = """
#     # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
#     predictions = model.predict(X_test)

#     # –û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏
#     accuracy = (predictions == y_test).mean()
#     print(f"–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {accuracy:.2f}")
#     """

#     # –°–±–æ—Ä–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–¥–∞
#     full_code = f"{base_code['imports']}\n{base_code['model_init']}\n{base_code['training']}\n{base_code['prediction']}"
#     return full_code

# def generate_params(params_dict):
#     """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç—Ä–æ–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏"""
#     return ", ".join([f"{k}={repr(v)}" for k, v in params_dict.items()])

# # –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
# def main():
#     st.title("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
#     st.markdown("""
#     **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
#     1. –£–∫–∞–∂–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏ –≤ –ª–µ–≤–æ–π –ø–∞–Ω–µ–ª–∏
#     2. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
#     3. –ò–∑—É—á–∏—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
#     """)

#     # –°–∞–π–¥–±–∞—Ä —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
#     with st.sidebar:
#         st.header("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—á–∏")
#         task_type = st.selectbox(
#             "–¢–∏–ø –∑–∞–¥–∞—á–∏",
#             ["classification", "regression", "clustering"]
#         )
#         data_size = st.selectbox(
#             "–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö",
#             ["small", "medium", "large", "very large"]
#         )
#         feature_types = st.multiselect(
#             "–¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
#             ["numeric", "categorical", "text", "images", "time_series"],
#             default=["numeric"]
#         )
#         interpretability = st.checkbox("–í–∞–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")
#         training_speed = st.checkbox("–í–∞–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è")
#         gpu_available = st.checkbox("–î–æ—Å—Ç—É–ø–µ–Ω GPU")
#         missing_data = st.checkbox("–ï—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö")

#     user_input = {
#         "task_type": task_type,
#         "data_size": data_size,
#         "feature_types": feature_types,
#         "interpretability": interpretability,
#         "training_speed": training_speed,
#         "gpu_available": gpu_available,
#         "missing_data": missing_data
#     }

#     # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
#     recommendations = recommend_algorithm(user_input, algorithms_db)

#     # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
#     st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")

#     if not recommendations:
#         st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞.")
#     else:
#         for algo, params in recommendations:
#             with st.expander(f"**{algo}**", expanded=False):
#                 st.write(f"**–¢–∏–ø –∑–∞–¥–∞—á–∏:** {', '.join(params['task_type'])}")
#                 st.write(f"**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** {params['interpretability']}")
#                 st.write(f"**–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:** {params['training_speed']}")
#                 st.write(f"**–ü–æ–¥–¥–µ—Ä–∂–∫–∞ GPU:** {params['gpu_support']}")
#                 st.write(f"**–†–∞–±–æ—Ç–∞ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏:** {params['handles_missing_data']}")

#                 if "hyperparameters" in params:
#                     hp = params["hyperparameters"]
#                     st.subheader("–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã")

#                     if "default" in hp:
#                         st.write("**–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**")
#                         st.code(hp["default"])

#                     if "important" in hp:
#                         st.write("**–ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**")
#                         for param, desc in hp["important"].items():
#                             st.write(f"- `{param}`: {desc}")

#                     if "tips" in hp:
#                         st.info(f"üí° {hp['tips']}")

#                 if "documentation" in params:
#                     st.markdown("**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:**")
#                     for name, url in params["documentation"].items():
#                         st.markdown(f"- [{name}]({url})", unsafe_allow_html=True)

#                 st.write(f"**–ü—Ä–∏–º–µ—á–∞–Ω–∏—è:** {params['notes']}")

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
def main():
    st.title("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
    st.markdown("""
    **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
    1. –£–∫–∞–∂–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏ –≤ –ª–µ–≤–æ–π –ø–∞–Ω–µ–ª–∏
    2. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
    3. –ò–∑—É—á–∏—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    """)

    # –°–∞–π–¥–±–∞—Ä —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    with st.sidebar:
        st.header("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—á–∏")
        task_type = st.selectbox(
            "–¢–∏–ø –∑–∞–¥–∞—á–∏",
            ["classification", "regression", "clustering"]
        )
        data_size = st.selectbox(
            "–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö",
            ["small", "medium", "large", "very large"]
        )
        feature_types = st.multiselect(
            "–¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
            ["numeric", "categorical", "text", "images", "time_series"],
            default=["numeric"]
        )
        interpretability = st.checkbox("–í–∞–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")
        training_speed = st.checkbox("–í–∞–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è")
        gpu_available = st.checkbox("–î–æ—Å—Ç—É–ø–µ–Ω GPU")
        missing_data = st.checkbox("–ï—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö")

    user_input = {
        "task_type": task_type,
        "data_size": data_size,
        "feature_types": feature_types,
        "interpretability": interpretability,
        "training_speed": training_speed,
        "gpu_available": gpu_available,
        "missing_data": missing_data
    }

    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    recommendations = recommend_algorithm(user_input, algorithms_db)

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")

    if not recommendations:
        st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞.")
    else:
      # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        st.subheader("üîç –í–∏–∑—É–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤")
        fig1, fig2, fig3 = visualize_algorithm_comparison(recommendations)

        st.plotly_chart(fig1, use_container_width=True)

        if fig2:
            st.plotly_chart(fig2, use_container_width=True)
        else:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")

        st.plotly_chart(fig3, use_container_width=True)

        # –î–µ—Ç–∞–ª–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É
        st.subheader("üìù –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º")
        for algo, params in recommendations:
            with st.expander(f"**{algo}**", expanded=False):
                st.write(f"**–¢–∏–ø –∑–∞–¥–∞—á–∏:** {', '.join(params['task_type'])}")
                st.write(f"**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** {params['interpretability']}")
                st.write(f"**–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:** {params['training_speed']}")
                st.write(f"**–ü–æ–¥–¥–µ—Ä–∂–∫–∞ GPU:** {params['gpu_support']}")
                st.write(f"**–†–∞–±–æ—Ç–∞ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏:** {params['handles_missing_data']}")

                if "hyperparameters" in params:
                    hp = params["hyperparameters"]
                    st.subheader("–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã")

                    if "default" in hp:
                        st.write("**–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**")
                        st.code(hp["default"])

                    if "important" in hp:
                        st.write("**–ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**")
                        for param, desc in hp["important"].items():
                            st.write(f"- `{param}`: {desc}")

                    if "tips" in hp:
                        st.info(f"üí° {hp['tips']}")

                if "documentation" in params:
                    st.markdown("**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:**")
                    for name, url in params["documentation"].items():
                        st.markdown(f"- [{name}]({url})", unsafe_allow_html=True)

                st.write(f"**–ü—Ä–∏–º–µ—á–∞–Ω–∏—è:** {params['notes']}")

        selected_algo = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞:",
            [algo[0] for algo in recommendations],
            index=0
        )

        # –ù–∞—Ö–æ–¥–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
        selected_params = None
        for algo, params in recommendations:
            if algo == selected_algo:
                selected_params = params
                break

        if selected_params:
            with st.expander(f"üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –¥–ª—è {selected_algo}", expanded=True):
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                if "hyperparameters" in selected_params:
                    tuned_params = create_parameter_tuner(selected_params["hyperparameters"])

                    # –ü—Ä–µ–≤—å—é –∏–∑–º–µ–Ω–µ–Ω–∏–π
                    st.subheader("–ò–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
                    st.json(tuned_params)

                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    generated_code = generate_algorithm_code(
                        selected_algo,
                        tuned_params,
                        selected_params
                    )
                else:
                    generated_code = generate_algorithm_code(
                        selected_algo,
                        {},
                        selected_params
                    )

                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–æ–¥–∞
                st.subheader("üßë‚Äçüíª –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥")
                st.code(generated_code, language='python')

                # –ö–Ω–æ–ø–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π
                col1, col2 = st.columns(2)
                with col1:
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å Python-—Å–∫—Ä–∏–ø—Ç",
                        data=generated_code,
                        file_name=f"{selected_algo.lower().replace(' ', '_')}_model.py",
                        mime="text/python"
                    )

                with col2:
                    if st.button("üìã –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥", key="copy_code"):
                        st.session_state.copied = True
                        st.experimental_set_query_params(copy_code=True)

                if st.session_state.get("copied", False):
                    st.success("–ö–æ–¥ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –±—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞!")

                # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
                st.markdown("""
                **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
                1. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—ã—à–µ (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
                2. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ –∫–æ–¥
                3. –í—Å—Ç–∞–≤—å—Ç–µ –≤ –≤–∞—à –ø—Ä–æ–µ–∫—Ç –∏ –∑–∞–º–µ–Ω–∏—Ç–µ `your_data.csv`
                4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                """)

if __name__ == "__main__":
    main()

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
# –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞–ª–∞: streamlit run ml_recommender_app.py
